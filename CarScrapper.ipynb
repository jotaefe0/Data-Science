{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import queue\n",
    "from threading import Thread as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://autos.mercadolibre.com.ar/autos_ITEM*CONDITION_2230581_NoIndex_True#unapplied_filter_id%3DCOLOR%26unapplied_filter_name%3DColor%26unapplied_value_id%3D52049%26unapplied_value_name%3DNegro%26unapplied_autoselect%3Dfalse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find_all('h2',attrs={\"class\":\"ui-search-item__title ui-search-item__group__element\"})\n",
    "title = [i.text for i in title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = soup.find_all('a', attrs={\"class\":\"ui-search-result__content ui-search-link\"})\n",
    "urls = [i.get('href') for i in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price = soup.find_all('span', attrs={\"class\":\"price-tag-fraction\"})#First 2 results are not valid! (At least in first page)\n",
    "# price = [i.text for i in price][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = etree.HTML(str(soup))\n",
    "price = dom.xpath('//div[@class=\"ui-search-result__content-wrapper\"]//span[@class=\"price-tag-fraction\"]')\n",
    "price = [i.text for i in price]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = dom.xpath('//a[@class=\"andes-pagination__link ui-search-link\"]')[0].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = int(soup.find('span', attrs={\"class\":\"andes-pagination__link\"}).text)#gets first page\n",
    "last = int(soup.find('li', attrs={\"class\":\"andes-pagination__page-count\"}).text.split(\" \")[1])#gets last page count, needs some formatting \"returns de 42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_list=[]\n",
    "title_list=[]\n",
    "price_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = \"https://autos.mercadolibre.com.ar/autos_ITEM*CONDITION_2230581_NoIndex_True#unapplied_filter_id%3DCOLOR%26unapplied_filter_name%3DColor%26unapplied_value_id%3D52049%26unapplied_value_name%3DNegro%26unapplied_autoselect%3Dfalse\"#init\n",
    "while True:\n",
    "    r = requests.get(next_page)\n",
    "    if r.status_code == 200:\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        \n",
    "        #title\n",
    "        title = soup.find_all('h2',attrs={\"class\":\"ui-search-item__title ui-search-item__group__element\"})\n",
    "        title = [i.text for i in title]\n",
    "        title_list.extend(title)\n",
    "        \n",
    "        #url\n",
    "        urls = soup.find_all('a', attrs={\"class\":\"ui-search-result__content ui-search-link\"})\n",
    "        urls = [i.get('href') for i in urls]\n",
    "        urls_list.extend(urls)\n",
    "       \n",
    "        #price\n",
    "        dom = etree.HTML(str(soup))\n",
    "        price = dom.xpath('//div[@class=\"ui-search-result__content-wrapper\"]//span[@class=\"price-tag-fraction\"]')\n",
    "        price = [i.text for i in price]\n",
    "        price_list.extend(price)\n",
    "        \n",
    "        #new pages\n",
    "        init = int(soup.find('span', attrs={\"class\":\"andes-pagination__link\"}).text)#gets first page\n",
    "        last = int(soup.find('li', attrs={\"class\":\"andes-pagination__page-count\"}).text.split(\" \")[1])#gets last page count, needs some formatting \"returns de 42\"\n",
    "    else:\n",
    "        break\n",
    "    print(init, last)\n",
    "    if init == last:\n",
    "        break\n",
    "    next_page = dom.xpath('//li[@class=\"andes-pagination__button andes-pagination__button--next\"]//a[@class=\"andes-pagination__link ui-search-link\"]')[0].get('href')#specify NEXT btn if not goes back and forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHecks that it got the same amount of data for each item\n",
    "print([len(urls_list),\n",
    "len(title_list),\n",
    "len(price_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Car\":title_list, \"url\":urls_list, \"price\":price_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This DataFrame only contains the data from the general used cars, if we want more data, we need to dive deeper, for example we can loop for each car brand in the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = ['Alfa-Romeo',\n",
    "'Audi',\n",
    "'BMW',\n",
    "'Chery',\n",
    "'Chevrolet',\n",
    "'Chrysler',\n",
    "'Citroen',\n",
    "'Dodge',\n",
    "'DS',\n",
    "'Fiat',\n",
    "'Ford',\n",
    "'Honda',\n",
    "'Hyundai',\n",
    "'Isuzu',\n",
    "'Jeep',\n",
    "'Kia',\n",
    "'Land-Rover',\n",
    "'Mazda',\n",
    "'Mercedes-Benz',\n",
    "'Mini',\n",
    "'Mitsubishi',\n",
    "'Nissan',\n",
    "'Peugeot',\n",
    "'Porsche',\n",
    "'RAM',\n",
    "'Renault',\n",
    "'Smart',\n",
    "'Subaru',\n",
    "'Suzuki',\n",
    "'Toyota',\n",
    "'Volkswagen',\n",
    "'Volvo']\n",
    "print(f\"We have {len(brands)} different brands\")\n",
    "brand_url = f\"https://autos.mercadolibre.com.ar/{brands[5]}/autos_ITEM*CONDITION_2230581_NoIndex_True#applied_filter_id%3DBRAND%26applied_filter_name%3DMarca%26applied_filter_order%3D2%26applied_value_id%3D60249%26applied_value_name%3DVolkswagen%26applied_value_order%3D36%26applied_value_results%3D18220%26is_custom%3Dfalse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we try to retireve more info from each of the 2592 cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given car\n",
    "given_url = \"https://auto.mercadolibre.com.ar/MLA-1125791437-renaul-duster-dynamique-16-ano-2016-as-automobili-_JM#position=1&search_layout=grid&type=item&tracking_id=bc9cdbb4-84a2-4ce2-a3ec-e18a58b86156\"\n",
    "r = requests.get(given_url)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = soup.find_all('span', attrs={\"class\":\"andes-table__column--value\"})#gets all the info we want with this class!!\n",
    "row = [i.text for i in row]\n",
    "col_titles = soup.find_all('th', attrs={\"class\":\"andes-table__header andes-table__header--left ui-pdp-specs__table__column ui-pdp-specs__table__column-title\"})#gets titles\n",
    "col_titles = [i.text for i in col_titles]\n",
    "#extra data (Location)\n",
    "location = soup.find_all('p', attrs={\"class\":\"ui-seller-info__status-info__subtitle\"})[2].text\n",
    "price = int(soup.find_all('span', attrs={\"class\":\"andes-money-amount__fraction\"})[0].text.replace(\".\",\"\"))#We also do some data cleaning here.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=col_titles)\n",
    "df_length = len(df)\n",
    "df.loc[df_length] = row\n",
    "df[\"Location\"] = location\n",
    "df[\"Price\"] = price\n",
    "df[\"Link\"] = given_url #In case we want to check it out, maybe we can find deals too\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, we got the data we want for 1 car, now we need it for all the cars we can get\n",
    "### The website lists more than 100000 cars, but it will only present 2592 per filter, so you need to scrap every filter..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We are going to combine the methods and scrap over all brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls_list=[]\n",
    "title_list=[]\n",
    "price_list=[]\n",
    "for i in range(len(brand_url)):\n",
    "    brand_url = f\"https://autos.mercadolibre.com.ar/{brands[i]}/autos_ITEM*CONDITION_2230581_NoIndex_True#applied_filter_id%3DBRAND%26applied_filter_name%3DMarca%26applied_filter_order%3D2%26applied_value_id%3D60249%26applied_value_name%3DVolkswagen%26applied_value_order%3D36%26applied_value_results%3D18220%26is_custom%3Dfalse\"\n",
    "    next_page = brand_url#init\n",
    "    print(brands[i])\n",
    "    while True:\n",
    "        r = requests.get(next_page)\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            print(r.status_code)\n",
    "               #title\n",
    "            title = soup.find_all('h2',attrs={\"class\":\"ui-search-item__title ui-search-item__group__element\"})\n",
    "            title = [i.text for i in title]\n",
    "            title_list.extend(title)\n",
    "\n",
    "            #url\n",
    "            urls = soup.find_all('a', attrs={\"class\":\"ui-search-result__content ui-search-link\"})\n",
    "            urls = [i.get('href') for i in urls]\n",
    "            urls_list.extend(urls)\n",
    "\n",
    "            #price\n",
    "            dom = etree.HTML(str(soup))\n",
    "            price = dom.xpath('//div[@class=\"ui-search-result__content-wrapper\"]//span[@class=\"price-tag-fraction\"]')\n",
    "            price = [i.text for i in price]\n",
    "            price_list.extend(price)\n",
    "            #new pages\n",
    "            init = int(soup.find('span', attrs={\"class\":\"andes-pagination__link\"}).text)#gets first page\n",
    "            last = int(soup.find('li', attrs={\"class\":\"andes-pagination__page-count\"}).text.split(\" \")[1])#gets last page count, needs some formatting \"returns de 42\"\n",
    "        else:\n",
    "            break\n",
    "        print(init, last)\n",
    "        if init == last:\n",
    "            break\n",
    "        next_page = dom.xpath('//li[@class=\"andes-pagination__button andes-pagination__button--next\"]//a[@class=\"andes-pagination__link ui-search-link\"]')[0].get('href')#specify NEXT btn if not goes back and forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://autos.mercadolibre.com.ar/autos_ITEM*CONDITION_2230581_NoIndex_True#filter\")\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "soup.find_all('a', attrs={\"class\":\"ui-search-search-modal-filter ui-search-link\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we get (after some time of scraping) Links to 33951 cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Car\":title_list, \"url\":urls_list, \"price\":price_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"car_urls\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"car_urls\")# to load in case there is a problem\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will iterate over this links to get the full data of each car\n",
    "### Note: We had to make some changes, since not always all data was available (e.g color) so it needed to match number of items in col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Given car\n",
    "#list_of_dicts = [] #I run this the first time then comment, since some times it crashes and need to start from were it left\n",
    "# for given_url in df[\"url\"][4705:]:\n",
    "def car(given_url):\n",
    "    r = requests.get(given_url)\n",
    "    r.status_code \n",
    "    print(f\"Code:{r.status_code} ok scrapping {given_url}\")\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    row = soup.find_all('span', attrs={\"class\":\"andes-table__column--value\"})#gets all the info we want with this class!!\n",
    "    row = [i.text for i in row]\n",
    "    if row == []:\n",
    "        return\n",
    "    \n",
    "    col_titles = soup.find_all('th', attrs={\"class\":\"andes-table__header andes-table__header--left ui-pdp-specs__table__column ui-pdp-specs__table__column-title\"})#gets titles\n",
    "    col_titles = [i.text for i in col_titles]\n",
    "    col_titles.append('Location')\n",
    "    col_titles.append('Price')\n",
    "    col_titles.append('Link')\n",
    "    \n",
    "    #extra data (Location)\n",
    "    location = soup.find_all('p', attrs={\"class\":\"ui-seller-info__status-info__subtitle\"})\n",
    "    if location == []:\n",
    "        print(\"no loc, passed\")\n",
    "        return\n",
    "        \n",
    "    location = location[-1].text\n",
    "    price = int(soup.find_all('span', attrs={\"class\":\"andes-money-amount__fraction\"})[0].text.replace(\".\",\"\"))#We also do some data cleaning here..\n",
    "    \n",
    "    row.append(location)\n",
    "    row.append(price)\n",
    "    row.append(given_url)\n",
    "    \n",
    "    kv_pairs = dict(zip(col_titles, row))\n",
    "    return kv_pairs\n",
    "\n",
    "# threads = [] \n",
    "# list2 = []\n",
    "# que = queue.Queue()\n",
    "# a = 4705\n",
    "# while a < 33951:\n",
    "#     threads = []\n",
    "#     for i in range(0,10):\n",
    "#         given_url = df[\"url\"][i+a]\n",
    "#         t = th(target=lambda q, arg1: q.put(car(arg1)), args=(que, given_url))\n",
    "#         t.start()\n",
    "#         threads.append(t)\n",
    "#         a = a + 1\n",
    "#     for t in threads:\n",
    "#         t.join()\n",
    "\n",
    "#         list2.append(que.get())\n",
    "#     print(f\" URL: {a} / 33951\")\n",
    "a = 8572\n",
    "for given_url in df[\"url\"][a::3]:# WE increment step by 3 because it scraps 1000 urls per hour, so we can start working, later we will scrap all\n",
    "    kv_pairs = car(given_url)\n",
    "    if kv_pairs == None:\n",
    "        continue\n",
    "    list_of_dicts.append(kv_pairs)\n",
    "    a = a + 3\n",
    "    print(f\" URL: {a} / 33951\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So threading wasnt optimizing speed, scrapping is time consuming, but is a one time operation so is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts_bk=list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list_of_dicts)#.to_csv(\"Autos_citroen_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnally we concat all df that we got into the final df and save it as csv, the result contains 15741 cars out of 33950 in the list, we can keep scraping the rest meanwhile, but this is ok as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "* Clean DF\n",
    "* Upload DF to AWS DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remaining URLS to scrap\n",
    "for link in da4[\"Link\"]:\n",
    "      print(link in df['url'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv(\"tosqldb.csv\")\n",
    "urls = urls[\"url\"]\n",
    "urls = urls.apply(lambda x : x[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Given car\n",
    "list_of_dicts = []\n",
    "# for given_url in df[\"url\"][4705:]:\n",
    "def car(given_url):\n",
    "    r = requests.get(given_url)\n",
    "    r.status_code \n",
    "    print(f\"Code:{r.status_code} ok scrapping {given_url}\")\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    row = soup.find_all('span', attrs={\"class\":\"andes-table__column--value\"})#gets all the info we want with this class!!\n",
    "    row = [i.text for i in row]\n",
    "    if row == []:\n",
    "        return\n",
    "    \n",
    "    col_titles = soup.find_all('th', attrs={\"class\":\"andes-table__header andes-table__header--left ui-pdp-specs__table__column ui-pdp-specs__table__column-title\"})#gets titles\n",
    "    col_titles = [i.text for i in col_titles]\n",
    "    col_titles.append('Location')\n",
    "    col_titles.append('Price')\n",
    "    col_titles.append('Link')\n",
    "    \n",
    "    #extra data (Location)\n",
    "    location = soup.find_all('p', attrs={\"class\":\"ui-seller-info__status-info__subtitle\"})\n",
    "    if location == []:\n",
    "        print(\"no loc, passed\")\n",
    "        return\n",
    "        \n",
    "    location = location[-1].text\n",
    "    price = int(soup.find_all('span', attrs={\"class\":\"andes-money-amount__fraction\"})[0].text.replace(\".\",\"\"))#We also do some data cleaning here..\n",
    "    \n",
    "    row.append(location)\n",
    "    row.append(price)\n",
    "    row.append(given_url)\n",
    "    \n",
    "    kv_pairs = dict(zip(col_titles, row))\n",
    "    return kv_pairs\n",
    "\n",
    "\n",
    "a = 0\n",
    "b = len(df[\"url\"].values) \n",
    "for given_url in df[\"url\"]:\n",
    "    if given_url[:50] in urls.values:\n",
    "        print(f\" URL: {given_url} Already scraped, continue\")\n",
    "        a = a + 1\n",
    "        continue\n",
    "    kv_pairs = car(given_url)\n",
    "    if kv_pairs == None:\n",
    "        continue\n",
    "    list_of_dicts.append(kv_pairs)\n",
    "    a = a + 1\n",
    "    print(f\" URL: {a} / {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.DataFrame(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.to_csv('last_scrap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv(\"tosqldb.csv\")\n",
    "urls = urls[\"url\"]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls.isin(df[\"url\"])\n",
    "df[\"url\"][0][:50] in urls.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = urls.apply(lambda x : x[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_url in urls.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
